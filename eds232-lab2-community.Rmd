---
title: "Lab 2 Community"
author: "Clarissa Boyajian"
date: "1/30/2022"
output: 
  html_document:
    number_sections: true
    code_folding: hide
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

# load R packages
librarian::shelf(tidyverse, DT, palmerpenguins, skimr, tibble, ggvoronoi, 
                 scales, cluster, vegan, vegan3d, factoextra, h2o)

# set seed for reproducible results
set.seed(42)
```


# Clustering

## Load and view `penguins` dataset

```{r}
# load the dataset
data("penguins")

# look at documentation in RStudio
if (interactive())
  help(penguins)

# show data table
datatable(penguins)
```

```{r}
# skim the table for a summary
skim(penguins)
```


## Plot the `penguins` dataset

```{r}
# remove the rows with NAs
penguins <- na.omit(penguins)

# plot bill length vs depth
ggplot(
  penguins, aes(bill_length_mm, bill_depth_mm)) +
  geom_point()
```

```{r}
# plot bill length vs depth, color by species
legend_pos <- theme(
    legend.position = c(0.95, 0.05),
    legend.justification = c("right", "bottom"),
    legend.box.just = "right")

ggplot(penguins, 
       aes(bill_length_mm, bill_depth_mm, color = species)) +
  geom_point() +
  legend_pos
```

## Cluster `penguins` data using kmeans()

```{r}
# cluster using kmeans
k <- 3  # number of clusters
penguins_k <- kmeans(penguins %>% 
                       select(bill_length_mm, bill_depth_mm), 
                     centers = k)

# show cluster result
penguins_k
```

```{r}
# compare clusters with species (which were not used to cluster)
table(penguins_k$cluster, penguins$species)
```

```{r}
# extract cluster assignment per observation
Cluster = factor(penguins_k$cluster)

ggplot(penguins, aes(x = bill_length_mm, 
                     y = bill_depth_mm, 
                     color = Cluster)) +
  geom_point() + 
  legend_pos
```

**Question**: Comparing the observed species plot with 3 species with the kmeans() cluster plot with 3 clusters, where does this “unsupervised” kmeans() technique (that does not use species to “fit” the model) produce similar versus different results? One or two sentences would suffice. Feel free to mention ranges of values along the axes.\
**Answer**: The observed species plot has 3 species that are generally split along diagonal lines, while the cluster plot has 3 clusters that are split along vertical lines. This means that the Adelie species bewteen ~45-55 along the x-axis and above ~17.5 along the y-axis are part of cluster 3.   \
FINISH

## Plot Voronoi diagram of clustered `penguins`

```{r}
# define bounding box for geom_voronoi()
xr <- extendrange(range(penguins$bill_length_mm), f = 0.1)
yr <- extendrange(range(penguins$bill_depth_mm), f = 0.1)
box <- tribble(
  ~bill_length_mm, ~bill_depth_mm, ~group,
  xr[1], yr[1], 1,
  xr[1], yr[2], 1,
  xr[2], yr[2], 1,
  xr[2], yr[1], 1,
  xr[1], yr[1], 1) %>% 
  data.frame()

# cluster using kmeans
k <- 3  # number of clusters
penguins_k <- kmeans(penguins %>% 
                       select(bill_length_mm, bill_depth_mm), 
                     centers = k)

# extract cluster assignment per observation
Cluster = factor(penguins_k$cluster)

# extract cluster centers
ctrs <- as.data.frame(penguins_k$centers) %>% 
  mutate(Cluster = factor(1:k))

# plot points with voronoi diagram showing nearest centroid
ggplot(penguins, aes(x = bill_length_mm, 
                     y = bill_depth_mm, 
                     color = Cluster)) +
  geom_point() + 
  legend_pos +
  geom_voronoi(data = ctrs, 
               aes(fill=Cluster), 
               color = NA, 
               alpha = 0.5, 
               outline = box) + 
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  geom_point(data = ctrs, 
             pch = 23, cex = 2, 
             fill = "black")
```


## Load `dune` dataset
```{r}
# load dune dataset from package vegan
data("dune")

# show documentation on dataset if interactive
if (interactive())
  help(dune)
```

## Calculate Ecological Distances on sites

```{r}
sites <- tribble(
  ~site, ~sp1, ~sp2, ~sp3,
    "A",    1,    1,    0,
    "B",    5,    5,    0,
    "C",    0,    0,    1) %>% 
  column_to_rownames("site")
sites
```

```{r}
sites_manhattan <- vegdist(sites, method = "manhattan") # sums all differences between prediciton and actual points (the shorter sum is the better predictor line)
sites_manhattan
```

```{r}
sites_euclidean <- vegdist(sites, method = "euclidean")
sites_euclidean
```

```{r}
sites_bray <- vegdist(sites, method = "bray")
sites_bray
```

**Question**: In your own words, how does Bray Curtis differ from Euclidean distance?\
**Answer**: Bray Curtis distance restricts the outputs between 0 and 1, with 0 being the most similar and 1 being the least similar. Whereas Euclidean distance is not restricted, making the outputs more impacted by abundance of species our outliers.\

## Agglomerative hierarchical clustering on dune

```{r}
# Dissimilarity matrix
d <- vegdist(dune, method = "bray")
dim(d)
```

```{r}
as.matrix(d)[1:5, 1:5]
```

```{r}
# Hierarchical clustering using Complete Linkage
hc1 <- hclust(d, method = "complete")

# Dendrogram plot of hc1
plot(hc1, cex = 0.6, hang = -1)
```

**Question**: Which function comes first, vegdist() or hclust(), and why?\
**Answer**: The function `vegdist()` comes before `hclust()` because you need to measure the distances (or dissimilarity values) first. You then use these values in order to create the clusters which become the dendrogram. \

```{r}
# Compute agglomerative clustering with agnes
hc2 <- agnes(dune, method = "complete")

# Agglomerative coefficient
hc2$ac
```

```{r}
# Dendrogram plot of hc2
plot(hc2, which.plot = 2)
```

```{r}
# methods to assess
m <- c("average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

# function to compute coefficient
ac <- function(x) {
  agnes(dune, method = x)$ac
}

# get agglomerative coefficient for each linkage method
purrr::map_dbl(m, ac)
```

```{r}
# Compute ward linkage clustering with agnes
hc3 <- agnes(dune, method = "ward")

# Agglomerative coefficient
hc3$ac
```

```{r}
# Dendrogram plot of hc3
plot(hc3, which.plot = 2)
```

**Question**: In your own words how does hclust() differ from agnes()?\
**Answer**: The `agnes()` and `hclust()` functions are very similar in many ways. Both perform hierarchical cluster analysis starting with each observation as its own object and working up the tree. The main difference is that `agnes()` also gives the Agglomerative Coefficient (AC). The AC is useful in describing the strength of the clustering structure.\
**Question**: Of the 4 methods, which is the “best” model in terms of Agglomerative Coefficient?\
**Answer**: The ward method creates the best model because it creates a model with the highest Agglomerative Coefficient (aka strongest clustering structure).\

## Divisive hierarchical clustering on dune

```{r}
# compute divisive hierarchical clustering
hc4 <- diana(dune)

# Divise coefficient; amount of clustering structure found
hc4$dc
```

**Question**: In your own words how does agnes() differ from diana()?\
**Answer**: The `agnes()` function is an agglomerative method, whereas the `diana()` function is a a divisive hierarchical method. This means that the dendrogram is created from the top down (creating clusters based on dissimilarities), rather than created from the bottom up (creating clusters based on similarity).\


## Determining optimal clusters

```{r}
# Plot cluster results
p1 <- fviz_nbclust(dune, FUN = hcut, method = "wss",  k.max = 10) +
  ggtitle("(A) Elbow method")

p2 <- fviz_nbclust(dune, FUN = hcut, method = "silhouette", k.max = 10) +
  ggtitle("(B) Silhouette method")

p3 <- fviz_nbclust(dune, FUN = hcut, method = "gap_stat", k.max = 10) +
  ggtitle("(C) Gap statistic")

# Display plots side by side
gridExtra::grid.arrange(p1, p2, p3, nrow = 1)
```

**Question**: How do the optimal number of clusters compare between methods for those with a dashed line?\
**Answer**: According to the Silhouette method the optimal number of clusters is 4. According to the Gap statistic method, the optimal number of clusters is 3. Because these numbers are similar, there isn't a clear specific answer but 3-4 clusters would be good.\
**Question**: In dendrogram plots, which is the biggest determinant of relatedness between observations: the distance between observations along the labeled axes or the height of their shared connection?\
**Answer**: The height of their shared connection is the biggest determinant of relatedness, because the height is the measure of distance between observations and clusters of observations. The distance on the labeled axes has no connection to measured distance, but rather shows which observations are clustered together. 

## Working with dendrograms

```{r}
# Construct dendorgram for the Ames housing example
hc5 <- hclust(d, method = "ward.D2" )
dend_plot <- fviz_dend(hc5)
dend_data <- attr(dend_plot, "dendrogram")
dend_cuts <- cut(dend_data, h = 8)
fviz_dend(dend_cuts$lower[[2]])
```

```{r}
# Ward's method
hc5 <- hclust(d, method = "ward.D2" )

# Cut tree into 4 groups
k = 4
sub_grp <- cutree(hc5, k = k)

# Number of members in each cluster
table(sub_grp)
```

```{r}
# Plot full dendogram
fviz_dend(
  hc5,
  k = k,
  horiz = TRUE,
  rect = TRUE,
  rect_fill = TRUE,
  rect_border = "jco",
  k_colors = "jco")
```


# Ordination

```{r}
# get data
url <- "https://koalaverse.github.io/homlr/data/my_basket.csv"
my_basket <- readr::read_csv(url)
dim(my_basket)

my_basket
```

## Principal Components Analysis (PCA)

```{r}
h2o.no_progress()  # turn off progress bars for brevity
h2o.init(max_mem_size = "5g")  # connect to H2O instance
```

```{r}
# convert data to h2o object
my_basket.h2o <- as.h2o(my_basket)

# run PCA
my_pca <- h2o.prcomp(
  training_frame = my_basket.h2o,
  pca_method = "GramSVD",
  k = ncol(my_basket.h2o), 
  transform = "STANDARDIZE", 
  impute_missing = TRUE,
  max_runtime_secs = 1000)
my_pca
```

**Question**: Why is the pca_method of “GramSVD” chosen over “GLRM”?\
**Answer**: The "GramSVD" method is used for data that is mostly composed of numeric variables, while the "GLRM" method is used for data composed of mostly categorical variables.\
**Question**: How many initial principal components are chosen with respect to dimensions of the input data?\
**Answer**: There are 42 initial principal components, which matches the number of features (aka the dimensions of the input data).\

```{r}
my_pca@model$eigenvectors %>% 
  as.data.frame() %>% 
  mutate(feature = row.names(.)) %>%
  ggplot(aes(pc1, reorder(feature, pc1))) +
  geom_point()
```

**Question**: What category of grocery items contribute most to PC1?\
**Answer**: The Bulmers cider contributes the most to PC1.\

```{r}
my_pca@model$eigenvectors %>% 
  as.data.frame() %>% 
  mutate(feature = row.names(.)) %>%
  ggplot(aes(pc1, pc2, label = feature)) +
  geom_text()
```

**Question**: What category of grocery items contribute the least to PC1 but positively towards PC2?\
**Answer**: Carrots contributes the least to PC but positively towards PC2.\

## Eigenvalue criterion

```{r}
# Compute eigenvalues
eigen <- my_pca@model$importance["Standard deviation", ] %>%
  as.vector() %>%
  .^2
  
# Sum of all eigenvalues equals number of variables
sum(eigen)
```

```{r}
## [1] 42

# Find PCs where the sum of eigenvalues is greater than or equal to 1
which(eigen >= 1)
```

```{r}
# Extract PVE and CVE
ve <- data.frame(
  PC  = my_pca@model$importance %>% seq_along(),
  PVE = my_pca@model$importance %>% .[2,] %>% unlist(),
  CVE = my_pca@model$importance %>% .[3,] %>% unlist())

# Plot PVE and CVE
ve %>%
  tidyr::gather(metric, variance_explained, -PC) %>%
  ggplot(aes(PC, variance_explained)) +
  geom_point() +
  facet_wrap(~ metric, ncol = 1, scales = "free") #+
  #geom_hline(yintercept = 0.9)
```

**Question**: How many principal components would you include to explain 90% of the total variance?\
**Answer**: I would include 36 principal components to explain 90% of the total variance.\

```{r}
# How many PCs required to explain at least 75% of total variability
min(which(ve$CVE >= 0.75))
```

```{r}
# Screee plot criterion
data.frame(
  PC  = my_pca@model$importance %>% seq_along,
  PVE = my_pca@model$importance %>% .[2,] %>% unlist()) %>%
  ggplot(aes(PC, PVE, group = 1, label = PC)) +
  geom_point() +
  geom_line() +
  geom_text(nudge_y = -.002)
```

**Question**: How many principal components to include up to the elbow of the PVE, i.e. the “elbow” before plateau of dimensions explaining the least variance?\
**Answer**: I would include 8 principal components, this is the last component before the "elbow" starts.\
**Question**: What are a couple of disadvantages to using PCA? \
**Answer**: PCA is highly impacted by outliers and does not work well for non-linear patterns.\


# Non-metric MultiDimensional Scaling (NMDS)

## Unconstrained Ordination on Species

```{r}
# vegetation and environment in lichen pastures from Vare et al (1995)
data("varespec") # species
data("varechem") # chemistry

varespec %>% tibble()
```

**Question**: What are the dimensions of the varespec data frame and what do rows versus columns represent?\
**Answer**: The columns in the varespec data frame represent 44 different species. The rows represent 24 different sites, with the values in each cell representing an estimated cover value for each species at each site. \

```{r}
vare.dis <- vegdist(varespec)
vare.mds0 <- monoMDS(vare.dis)
stressplot(vare.mds0)
```

**Question**: The “stress” in a stressplot represents the difference between the observed input distance versus the fitted ordination distance. How much better is the non-metric (i.e., NMDS) fit versus a linear fit (as with PCA) in terms of \(R^2\)?\
**Answer**: The non-metric fit is better than the linear fit by a difference of 0.104 in the \(R^2\) value.\

```{r}
ordiplot(vare.mds0, type = "t")
```

**Question**: What two sites are most dissimilar based on species composition for the first component MDS1? And two more most dissimilar sites for the second component MDS2?\
**Answer**: The two sites that are most dissimilar based on species composition for the first component MDS1 are sites 28 and 4 or 5. The two most dissimilar sites for hte second component MDS2 are 21 and 5.\

```{r}
vare.mds <- metaMDS(varespec, trace = FALSE)
vare.mds
```

```{r}
plot(vare.mds, type = "t")
```

**Question**: What is the basic difference between metaMDS and monoMDS()?\
**Answer**: `metaMDS()` uses `monoMDS()` to create a nonlinear regression from multiple different random starts.\

## Overlay with Environment

```{r}
ef <- envfit(vare.mds, varechem, permu = 999)
ef
```

```{r}
plot(vare.mds, display = "sites")
plot(ef, p.max = 0.05)
```

**Question**: What two soil chemistry elements have the strongest negative relationship with NMDS1 that is based on species composition?\
**Answer**: Aluminium (Al) and Iron (Fe) have the strongest negative relationship with NMDS1.\

```{r}
ef <- envfit(vare.mds ~ Al + Ca, data = varechem)
plot(vare.mds, display = "sites")
plot(ef)

tmp <- with(varechem, ordisurf(vare.mds, Al, add = TRUE))
ordisurf(vare.mds ~ Ca, data = varechem, add = TRUE, col = "green4")
```

**Question**: Which of the two NMDS axes differentiates Ca the most, i.e. has the highest value given by the contours at the end (and not middle) of the axis?\
**Answer**: \

## Constrained Ordination on Species and Environment

```{r}
# ordinate on species constrained by three soil elements
vare.cca <- cca(varespec ~ Al + P + K, varechem)
vare.cca
```

```{r}
# plot ordination
plot(vare.cca)
```

**Question**: What is the difference between “constrained” versus “unconstrained” ordination within ecological context?\
**Answer**: \

```{r}
# plot 3 dimensions
ordiplot3d(vare.cca, type = "h")
```

```{r}
if (interactive()){
  ordirgl(vare.cca)
}
```

**Question**: What sites are most differentiated by CCA1, i.e. furthest apart along its axis, based on species composition AND the environmnent? What is the strongest environmental vector for CCA1, i.e. longest environmental vector in the direction of the CCA1 axes?\
**Answer**: \